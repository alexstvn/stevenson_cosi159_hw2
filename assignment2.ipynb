{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7754616,"sourceType":"datasetVersion","datasetId":4534282},{"sourceId":7754621,"sourceType":"datasetVersion","datasetId":4534286},{"sourceId":7754860,"sourceType":"datasetVersion","datasetId":4534447}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nimport argparse\n\nimport sys\nsys.path.append( \"/kaggle/input/assignment2input\" )\n# from model import SphereCNN\n# from dataloader import LFW4Training, LFW4Eval\n# from parser import parse_args # suggestion from vs code\nfrom utils import set_seed, AverageMeter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-08T23:45:21.355892Z","iopub.execute_input":"2024-03-08T23:45:21.356609Z","iopub.status.idle":"2024-03-08T23:45:21.362964Z","shell.execute_reply.started":"2024-03-08T23:45:21.356531Z","shell.execute_reply":"2024-03-08T23:45:21.362034Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# parser.py","metadata":{}},{"cell_type":"code","source":"def parse_args(args=None):\n    parser = argparse.ArgumentParser(description=\"SphereFace\")\n\n    parser.add_argument('--seed', type=int, default=2021)\n    parser.add_argument('--device', type=str, default=\"cuda:0\")\n\n    parser.add_argument('--batch_size', type=int, default=128) # batch size = 128 due to pg. 6 in CNNs Setup\n    parser.add_argument('--epoch', type=int, default=100)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--eval_interval', type=int, default=20)\n\n    # EDITING TO USE KAGGLE INPUT DATA\n    parser.add_argument('--train_file', type=str, default=\"/kaggle/input/assignment2input/pairsDevTrain.txt\")\n    parser.add_argument('--eval_file', type=str, default=\"/kaggle/input/assignment2input/pairsDevTest.txt\")\n    parser.add_argument('--img_folder', type=str, default=\"/kaggle/input/lfwdata/lfw\")\n\n    if args is None:\n      args=[]\n    args = parser.parse_args(args)\n    return args","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.364606Z","iopub.execute_input":"2024-03-08T23:45:21.364864Z","iopub.status.idle":"2024-03-08T23:45:21.376855Z","shell.execute_reply.started":"2024-03-08T23:45:21.364842Z","shell.execute_reply":"2024-03-08T23:45:21.376091Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# model.py","metadata":{}},{"cell_type":"markdown","source":"Design of Neural Network architecture.","metadata":{}},{"cell_type":"markdown","source":"## A-Softmax Loss Function","metadata":{}},{"cell_type":"markdown","source":"This implements the loss function (A-softmax Loss).","metadata":{}},{"cell_type":"code","source":"class AngularPenaltySMLoss(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-7, m=None):\n        super(AngularPenaltySMLoss, self).__init__()\n\n        self.m = 4. if not m else m\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.fc = nn.Linear(in_features, out_features, bias=False)\n        self.eps = eps\n\n    def forward(self, x, labels):\n        '''\n        input shape (N, in_features)\n        '''\n        assert len(x) == len(labels)\n        assert torch.min(labels) >= 0\n        assert torch.max(labels) < self.out_features\n\n        # normalizes weights of linear layer\n        for W in self.fc.parameters():\n            W = F.normalize(W, p=2, dim=1)\n\n        x = F.normalize(x, p=2, dim=1)\n\n        wf = self.fc(x)\n\n        # calculates numerator of loss function\n        numerator = torch.cos(self.m * torch.acos(\n            torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1. + self.eps, 1 - self.eps)))\n\n        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y + 1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n        \n        # calculates denominator of loss function\n        denominator = torch.exp(numerator) + torch.sum(torch.exp(excl), dim=1)\n        L = numerator - torch.log(denominator)\n\n        return -torch.mean(L)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.378022Z","iopub.execute_input":"2024-03-08T23:45:21.378380Z","iopub.status.idle":"2024-03-08T23:45:21.397982Z","shell.execute_reply.started":"2024-03-08T23:45:21.378348Z","shell.execute_reply":"2024-03-08T23:45:21.397207Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class SphereCNN(nn.Module):\n    def __init__(self, class_num: int, feature=False):\n        super(SphereCNN, self).__init__()\n        self.class_num = class_num\n        self.feature = feature\n\n        # 4-LAYER CONVOLUTIONAL NEURAL NETWORK\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=2)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2)\n\n        self.fc5 = nn.Linear(512 * 5 * 5, 512)\n        self.angular = AngularPenaltySMLoss(512, self.class_num) # A-Softmax Loss\n\n    def forward(self, x, y):\n        # 4-Layer Convolution Network\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))  # batch_size (0) * out_channels (1) * height (2) * width (3)\n\n        x = x.view(x.size(0), -1)  # batch_size (0) * (out_channels * height * width)\n        x = self.fc5(x)\n\n        if self.feature or y is None:\n            return x\n        else:\n            x_angle = self.angular(x, y)\n            return x, x_angle\n","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.398947Z","iopub.execute_input":"2024-03-08T23:45:21.399221Z","iopub.status.idle":"2024-03-08T23:45:21.412032Z","shell.execute_reply.started":"2024-03-08T23:45:21.399198Z","shell.execute_reply":"2024-03-08T23:45:21.411195Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\nnet = SphereCNN(50)\ninput = torch.ones(64, 3, 96, 96)\noutput = net(input, None)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.414599Z","iopub.execute_input":"2024-03-08T23:45:21.414920Z","iopub.status.idle":"2024-03-08T23:45:21.580226Z","shell.execute_reply.started":"2024-03-08T23:45:21.414890Z","shell.execute_reply":"2024-03-08T23:45:21.579185Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# dataloader.py","metadata":{}},{"cell_type":"markdown","source":"<code>import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms</code>","metadata":{}},{"cell_type":"markdown","source":"## LFW4Training(Dataset)","metadata":{}},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"class LFW4Training(Dataset):\n    def __init__(self, train_file: str, img_folder: str):\n        self.img_folder = img_folder\n\n        names = os.listdir(img_folder)\n        self.name2label = {name: idx for idx, name in enumerate(names)}\n        self.n_label = len(self.name2label)\n\n        with open(train_file) as f:\n            train_meta_info = f.read().splitlines()\n\n        self.train_list = []\n        for line in train_meta_info:\n            line = line.split(\"\\t\")\n            if len(line) == 3:\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"))\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[2]).zfill(4) + \".jpg\"))\n            elif len(line) == 4:\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"))\n                self.train_list.append(os.path.join(line[2], line[2] + \"_\" + str(line[3]).zfill(4) + \".jpg\"))\n            else:\n                pass\n\n        self.transform = transforms.Compose([\n            transforms.Resize(96),\n            transforms.RandomHorizontalFlip(), # DATA AUGMENTATION - horizontally flipped as in pg.6\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5]),\n        ])\n\n    def __getitem__(self, index):\n        img_path = self.train_list[index]\n\n        img = Image.open(os.path.join(self.img_folder, img_path))\n        img = self.transform(img)\n\n        name = img_path.split(\"/\")[0]\n        label = self.name2label[name]\n\n        return img, label\n\n    def __len__(self):\n        return len(self.train_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.581482Z","iopub.execute_input":"2024-03-08T23:45:21.581854Z","iopub.status.idle":"2024-03-08T23:45:21.594809Z","shell.execute_reply.started":"2024-03-08T23:45:21.581829Z","shell.execute_reply":"2024-03-08T23:45:21.593886Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## LFW4Eval(Dataset)","metadata":{}},{"cell_type":"code","source":"class LFW4Eval(Dataset):\n    def __init__(self, eval_file: str, img_folder: str):\n        self.img_folder = img_folder\n\n        with open(eval_file) as f:\n            eval_meta_info = f.read().splitlines()\n\n        self.eval_list = []\n        for line in eval_meta_info:\n            line = line.split(\"\\t\")\n            if len(line) == 3:\n                eval_pair = (\n                    os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"),\n                    os.path.join(line[0], line[0] + \"_\" + str(line[2]).zfill(4) + \".jpg\"),\n                    1,\n                )\n                self.eval_list.append(eval_pair)\n            elif len(line) == 4:\n                eval_pair = (\n                    os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"),\n                    os.path.join(line[2], line[2] + \"_\" + str(line[3]).zfill(4) + \".jpg\"),\n                    0,\n                )\n                self.eval_list.append(eval_pair)\n            else:\n                pass\n\n        self.transform = transforms.Compose([\n            transforms.Resize(96),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5]),\n        ])\n\n    def __getitem__(self, index):\n        img_1_path, img_2_path, label = self.eval_list[index]\n\n        img_1 = Image.open(os.path.join(self.img_folder, img_1_path))\n        img_2 = Image.open(os.path.join(self.img_folder, img_2_path))\n        img_1 = self.transform(img_1)\n        img_2 = self.transform(img_2)\n\n        return img_1, img_2, label\n\n    def __len__(self):\n        return len(self.eval_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.595997Z","iopub.execute_input":"2024-03-08T23:45:21.596271Z","iopub.status.idle":"2024-03-08T23:45:21.612377Z","shell.execute_reply.started":"2024-03-08T23:45:21.596248Z","shell.execute_reply":"2024-03-08T23:45:21.611464Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# main.py","metadata":{}},{"cell_type":"code","source":"# TESTING AND RESULTS\n\ndef eval(data_loader: DataLoader, model: SphereCNN, device: torch.device, threshold: float = 0.5):\n    model.eval()\n    model.feature = True\n    sim_func = nn.CosineSimilarity()\n\n    cnt = 0.\n    total = 0.\n\n    t1 = time.time()\n    with torch.no_grad():\n        for img_1, img_2, label in data_loader:\n            img_1 = img_1.to(device)\n            img_2 = img_2.to(device)\n            label = label.to(device)\n\n            feat_1 = model(img_1, None)\n            feat_2 = model(img_2, None)\n            sim = sim_func(feat_1, feat_2)\n\n            sim[sim > threshold] = 1\n            sim[sim <= threshold] = 0\n\n            total += sim.size(0)\n            for i in range(sim.size(0)):\n                if sim[i] == label[i]:\n                    cnt += 1\n\n    print(\"Acc.: %.4f; Time: %.3f\" % (cnt / total, time.time() - t1))\n    return","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.613694Z","iopub.execute_input":"2024-03-08T23:45:21.614501Z","iopub.status.idle":"2024-03-08T23:45:21.630064Z","shell.execute_reply.started":"2024-03-08T23:45:21.614467Z","shell.execute_reply":"2024-03-08T23:45:21.629312Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"args = parse_args()\n\nset_seed(args.seed)\ndevice = torch.device(args.device)\n\n# DATA LOADING\n# size determination of train/validation sets\ntrain_set_og = LFW4Training(args.train_file, args.img_folder)\n\nvalidation_split = 0.2 # 20% of training data \ntrain_size = len(train_set_og)\nval_size = int(validation_split * train_size)\ntrain_size = train_size - val_size\n\n# creating datasets\n# train_set = LFW4Training(args.train_file, args.img_folder)\ntrain_set, val_set = torch.utils.data.random_split(train_set_og, [train_size, val_size])\neval_set = LFW4Eval(args.eval_file, args.img_folder)\n\n# creating data loader\ntrain_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=args.batch_size)\neval_loader = DataLoader(eval_set, batch_size=args.batch_size)\n\n\n# INITIALIZE NEURAL NETWORK\nmodel = SphereCNN(class_num=train_set_og.n_label)\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\nloss_record = AverageMeter()\n\n# TRAINING DATA\nfor epoch in range(args.epoch):\n    t1 = time.time()\n    model.train()\n    model.feature = False\n    loss_record.reset()\n\n    for inputs, targets in train_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        _, loss = model(inputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        loss_record.update(loss)\n\n    print(\"Epoch: %s; Loss: %.3f; Time: %.3f\" % (str(epoch).zfill(2), loss_record.avg, time.time() - t1))\n\n    if (epoch + 1) % args.eval_interval == 0:\n        torch.save(model.state_dict(), \"/spherecnn.pth\")\n        eval(eval_loader, model, device)\n        \n        #VALIDATION - attempting to validate data\n#         val_loss = 0.0\n#         model.eval()\n#         with torch.no_grad():\n#             for inputs, targets in val_loader:\n#                 inputs = inputs.to(device)\n#                 targets = targets.to(device)\n\n#                 outputs, loss = model(inputs, targets)\n#                 val_loss += loss.item() * inputs.size(0)\n#         val_loss /= len(val_set)\n#         print(\"Validation Loss: %.3f\" % val_loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-08T23:45:21.631399Z","iopub.execute_input":"2024-03-08T23:45:21.631740Z","iopub.status.idle":"2024-03-09T00:00:28.437248Z","shell.execute_reply.started":"2024-03-08T23:45:21.631711Z","shell.execute_reply":"2024-03-09T00:00:28.436340Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch: 00; Loss: 7.629; Time: 10.301\nEpoch: 01; Loss: 7.388; Time: 8.811\nEpoch: 02; Loss: 7.109; Time: 8.842\nEpoch: 03; Loss: 6.923; Time: 8.534\nEpoch: 04; Loss: 6.812; Time: 8.778\nEpoch: 05; Loss: 6.741; Time: 8.846\nEpoch: 06; Loss: 6.697; Time: 8.908\nEpoch: 07; Loss: 6.663; Time: 9.101\nEpoch: 08; Loss: 6.637; Time: 8.868\nEpoch: 09; Loss: 6.613; Time: 8.601\nEpoch: 10; Loss: 6.589; Time: 8.674\nEpoch: 11; Loss: 6.564; Time: 8.839\nEpoch: 12; Loss: 6.536; Time: 8.891\nEpoch: 13; Loss: 6.508; Time: 8.862\nEpoch: 14; Loss: 6.478; Time: 9.222\nEpoch: 15; Loss: 6.447; Time: 8.831\nEpoch: 16; Loss: 6.415; Time: 8.790\nEpoch: 17; Loss: 6.385; Time: 8.613\nEpoch: 18; Loss: 6.352; Time: 8.842\nEpoch: 19; Loss: 6.318; Time: 8.923\nAcc.: 0.5500; Time: 4.605\nEpoch: 20; Loss: 6.284; Time: 9.144\nEpoch: 21; Loss: 6.249; Time: 8.941\nEpoch: 22; Loss: 6.216; Time: 8.639\nEpoch: 23; Loss: 6.184; Time: 8.525\nEpoch: 24; Loss: 6.150; Time: 8.793\nEpoch: 25; Loss: 6.117; Time: 8.640\nEpoch: 26; Loss: 6.083; Time: 8.606\nEpoch: 27; Loss: 6.048; Time: 8.834\nEpoch: 28; Loss: 6.013; Time: 8.893\nEpoch: 29; Loss: 5.979; Time: 8.710\nEpoch: 30; Loss: 5.944; Time: 8.579\nEpoch: 31; Loss: 5.911; Time: 8.674\nEpoch: 32; Loss: 5.878; Time: 8.613\nEpoch: 33; Loss: 5.843; Time: 8.784\nEpoch: 34; Loss: 5.811; Time: 8.887\nEpoch: 35; Loss: 5.787; Time: 8.841\nEpoch: 36; Loss: 5.750; Time: 8.778\nEpoch: 37; Loss: 5.713; Time: 8.750\nEpoch: 38; Loss: 5.678; Time: 8.699\nEpoch: 39; Loss: 5.644; Time: 8.624\nAcc.: 0.6180; Time: 4.637\nEpoch: 40; Loss: 5.609; Time: 8.661\nEpoch: 41; Loss: 5.576; Time: 8.557\nEpoch: 42; Loss: 5.550; Time: 8.719\nEpoch: 43; Loss: 5.520; Time: 8.752\nEpoch: 44; Loss: 5.489; Time: 9.003\nEpoch: 45; Loss: 5.456; Time: 8.813\nEpoch: 46; Loss: 5.423; Time: 8.780\nEpoch: 47; Loss: 5.390; Time: 8.780\nEpoch: 48; Loss: 5.355; Time: 8.614\nEpoch: 49; Loss: 5.323; Time: 8.630\nEpoch: 50; Loss: 5.287; Time: 8.775\nEpoch: 51; Loss: 5.258; Time: 8.959\nEpoch: 52; Loss: 5.231; Time: 8.770\nEpoch: 53; Loss: 5.202; Time: 8.454\nEpoch: 54; Loss: 5.169; Time: 8.815\nEpoch: 55; Loss: 5.135; Time: 8.730\nEpoch: 56; Loss: 5.101; Time: 8.579\nEpoch: 57; Loss: 5.069; Time: 8.870\nEpoch: 58; Loss: 5.047; Time: 8.579\nEpoch: 59; Loss: 5.017; Time: 8.726\nAcc.: 0.6380; Time: 4.452\nEpoch: 60; Loss: 4.988; Time: 8.793\nEpoch: 61; Loss: 4.958; Time: 8.567\nEpoch: 62; Loss: 4.927; Time: 8.635\nEpoch: 63; Loss: 4.893; Time: 8.526\nEpoch: 64; Loss: 4.856; Time: 8.842\nEpoch: 65; Loss: 4.816; Time: 8.590\nEpoch: 66; Loss: 4.784; Time: 8.619\nEpoch: 67; Loss: 4.754; Time: 8.743\nEpoch: 68; Loss: 4.733; Time: 8.494\nEpoch: 69; Loss: 4.714; Time: 8.562\nEpoch: 70; Loss: 4.693; Time: 8.573\nEpoch: 71; Loss: 4.661; Time: 8.941\nEpoch: 72; Loss: 4.617; Time: 8.649\nEpoch: 73; Loss: 4.585; Time: 8.717\nEpoch: 74; Loss: 4.552; Time: 8.721\nEpoch: 75; Loss: 4.520; Time: 8.694\nEpoch: 76; Loss: 4.481; Time: 8.846\nEpoch: 77; Loss: 4.447; Time: 8.711\nEpoch: 78; Loss: 4.416; Time: 9.137\nEpoch: 79; Loss: 4.405; Time: 9.070\nAcc.: 0.6570; Time: 4.647\nEpoch: 80; Loss: 4.383; Time: 9.145\nEpoch: 81; Loss: 4.369; Time: 8.698\nEpoch: 82; Loss: 4.332; Time: 8.661\nEpoch: 83; Loss: 4.293; Time: 8.619\nEpoch: 84; Loss: 4.249; Time: 8.827\nEpoch: 85; Loss: 4.225; Time: 8.741\nEpoch: 86; Loss: 4.193; Time: 8.925\nEpoch: 87; Loss: 4.162; Time: 9.327\nEpoch: 88; Loss: 4.132; Time: 8.764\nEpoch: 89; Loss: 4.102; Time: 8.919\nEpoch: 90; Loss: 4.071; Time: 9.115\nEpoch: 91; Loss: 4.044; Time: 8.701\nEpoch: 92; Loss: 4.026; Time: 9.326\nEpoch: 93; Loss: 4.000; Time: 9.454\nEpoch: 94; Loss: 3.979; Time: 8.649\nEpoch: 95; Loss: 3.946; Time: 8.534\nEpoch: 96; Loss: 3.910; Time: 9.173\nEpoch: 97; Loss: 3.874; Time: 8.884\nEpoch: 98; Loss: 3.849; Time: 8.813\nEpoch: 99; Loss: 3.827; Time: 8.885\nAcc.: 0.6470; Time: 4.742\n","output_type":"stream"}]}]}