{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7754616,"sourceType":"datasetVersion","datasetId":4534282},{"sourceId":7754621,"sourceType":"datasetVersion","datasetId":4534286},{"sourceId":7754860,"sourceType":"datasetVersion","datasetId":4534447}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nimport argparse\n\nimport sys\nsys.path.append( \"/kaggle/input/assignment2input\" ) # necessary if using kaggle input files\n# from model import SphereCNN\n# from dataloader import LFW4Training, LFW4Eval\n# from parser import parse_args # suggestion from vs code\nfrom utils import set_seed, AverageMeter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-16T02:03:28.100375Z","iopub.execute_input":"2024-03-16T02:03:28.100691Z","iopub.status.idle":"2024-03-16T02:03:35.683253Z","shell.execute_reply.started":"2024-03-16T02:03:28.100664Z","shell.execute_reply":"2024-03-16T02:03:35.682351Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# parser.py","metadata":{}},{"cell_type":"markdown","source":"This class allows us to select and tune the hyperparameters.","metadata":{}},{"cell_type":"code","source":"def parse_args(args=None):\n    parser = argparse.ArgumentParser(description=\"SphereFace\")\n\n    parser.add_argument('--seed', type=int, default=2021)\n    parser.add_argument('--device', type=str, default=\"cuda:0\")\n\n    parser.add_argument('--batch_size', type=int, default=128) # batch size = 128 due to pg. 6 in CNNs Setup\n    parser.add_argument('--epoch', type=int, default=100)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--eval_interval', type=int, default=20)\n\n    # EDITING TO USE KAGGLE INPUT DATA\n    parser.add_argument('--train_file', type=str, default=\"/kaggle/input/assignment2input/pairsDevTrain.txt\")\n    parser.add_argument('--eval_file', type=str, default=\"/kaggle/input/assignment2input/pairsDevTest.txt\")\n    parser.add_argument('--img_folder', type=str, default=\"/kaggle/input/lfwdata/lfw\")\n\n    if args is None:\n      args=[]\n    args = parser.parse_args(args)\n    return args","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:35.691455Z","iopub.execute_input":"2024-03-16T02:03:35.691761Z","iopub.status.idle":"2024-03-16T02:03:35.699891Z","shell.execute_reply.started":"2024-03-16T02:03:35.691735Z","shell.execute_reply":"2024-03-16T02:03:35.698692Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# model.py","metadata":{}},{"cell_type":"markdown","source":"Design of Neural Network architecture.\n\n`import torch\nfrom torch import nn\nimport torch.nn.functional as F`","metadata":{}},{"cell_type":"markdown","source":"## A-Softmax Loss Function","metadata":{}},{"cell_type":"markdown","source":"This implements the loss function (A-softmax Loss).","metadata":{}},{"cell_type":"code","source":"class AngularPenaltySMLoss(nn.Module):\n    def __init__(self, in_features, out_features, eps=1e-7, m=None):\n        super(AngularPenaltySMLoss, self).__init__()\n\n        self.m = 4. if not m else m\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.fc = nn.Linear(in_features, out_features, bias=False)\n        self.eps = eps\n\n    def forward(self, x, labels):\n        '''\n        input shape (N, in_features)\n        '''\n        assert len(x) == len(labels)\n        assert torch.min(labels) >= 0\n        assert torch.max(labels) < self.out_features\n\n        # normalizes weights of linear layer\n        for W in self.fc.parameters():\n            W = F.normalize(W, p=2, dim=1)\n\n        x = F.normalize(x, p=2, dim=1)\n\n        wf = self.fc(x)\n\n        # calculates numerator of loss function\n        numerator = torch.cos(self.m * torch.acos(\n            torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1. + self.eps, 1 - self.eps)))\n\n        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y + 1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n        \n        # calculates denominator of loss function\n        denominator = torch.exp(numerator) + torch.sum(torch.exp(excl), dim=1)\n        L = numerator - torch.log(denominator)\n\n        return -torch.mean(L)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:35.700988Z","iopub.execute_input":"2024-03-16T02:03:35.701311Z","iopub.status.idle":"2024-03-16T02:03:35.714092Z","shell.execute_reply.started":"2024-03-16T02:03:35.701287Z","shell.execute_reply":"2024-03-16T02:03:35.713352Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## SphereFace CNN Architecture\nThis class implements a 4-layer Convolutional Neural Network for SphereFace.","metadata":{}},{"cell_type":"code","source":"class SphereCNN(nn.Module):\n    def __init__(self, class_num: int, feature=False):\n        super(SphereCNN, self).__init__()\n        self.class_num = class_num\n        self.feature = feature\n\n        # 4-LAYER CONVOLUTIONAL NEURAL NETWORK\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=2)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2)\n\n        self.fc5 = nn.Linear(512 * 5 * 5, 512)\n        self.angular = AngularPenaltySMLoss(512, self.class_num) # A-Softmax Loss\n\n    def forward(self, x, y):\n        # 4-Layer Convolution Network\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))  # batch_size (0) * out_channels (1) * height (2) * width (3)\n\n        x = x.view(x.size(0), -1)  # batch_size (0) * (out_channels * height * width)\n        x = self.fc5(x)\n\n        if self.feature or y is None:\n            return x\n        else:\n            x_angle = self.angular(x, y)\n            return x, x_angle\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:35.715343Z","iopub.execute_input":"2024-03-16T02:03:35.715743Z","iopub.status.idle":"2024-03-16T02:03:35.730736Z","shell.execute_reply.started":"2024-03-16T02:03:35.715713Z","shell.execute_reply":"2024-03-16T02:03:35.729996Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\nnet = SphereCNN(50)\ninput = torch.ones(64, 3, 96, 96)\noutput = net(input, None)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:35.732018Z","iopub.execute_input":"2024-03-16T02:03:35.732648Z","iopub.status.idle":"2024-03-16T02:03:36.122402Z","shell.execute_reply.started":"2024-03-16T02:03:35.732622Z","shell.execute_reply":"2024-03-16T02:03:36.121386Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# dataloader.py\n<code>import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms</code>","metadata":{}},{"cell_type":"markdown","source":"## LFW4Training(Dataset)","metadata":{}},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"class LFW4Training(Dataset):\n    def __init__(self, train_file: str, img_folder: str):\n        self.img_folder = img_folder\n\n        names = os.listdir(img_folder)\n        self.name2label = {name: idx for idx, name in enumerate(names)}\n        self.n_label = len(self.name2label)\n\n        with open(train_file) as f:\n            train_meta_info = f.read().splitlines()\n\n        self.train_list = []\n        for line in train_meta_info:\n            line = line.split(\"\\t\")\n            if len(line) == 3:\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"))\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[2]).zfill(4) + \".jpg\"))\n            elif len(line) == 4:\n                self.train_list.append(os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"))\n                self.train_list.append(os.path.join(line[2], line[2] + \"_\" + str(line[3]).zfill(4) + \".jpg\"))\n            else:\n                pass\n\n        self.transform = transforms.Compose([\n            transforms.Resize(96),\n            transforms.RandomHorizontalFlip(), # DATA AUGMENTATION - horizontally flipped as in pg.6\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5]),\n        ])\n\n    def __getitem__(self, index):\n        img_path = self.train_list[index]\n\n        img = Image.open(os.path.join(self.img_folder, img_path))\n        img = self.transform(img)\n\n        name = img_path.split(\"/\")[0]\n        label = self.name2label[name]\n\n        return img, label\n\n    def __len__(self):\n        return len(self.train_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:36.123816Z","iopub.execute_input":"2024-03-16T02:03:36.124145Z","iopub.status.idle":"2024-03-16T02:03:36.138185Z","shell.execute_reply.started":"2024-03-16T02:03:36.124120Z","shell.execute_reply":"2024-03-16T02:03:36.137167Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## LFW4Eval(Dataset)","metadata":{}},{"cell_type":"code","source":"class LFW4Eval(Dataset):\n    def __init__(self, eval_file: str, img_folder: str):\n        self.img_folder = img_folder\n\n        with open(eval_file) as f:\n            eval_meta_info = f.read().splitlines()\n\n        self.eval_list = []\n        for line in eval_meta_info:\n            line = line.split(\"\\t\")\n            if len(line) == 3:\n                eval_pair = (\n                    os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"),\n                    os.path.join(line[0], line[0] + \"_\" + str(line[2]).zfill(4) + \".jpg\"),\n                    1,\n                )\n                self.eval_list.append(eval_pair)\n            elif len(line) == 4:\n                eval_pair = (\n                    os.path.join(line[0], line[0] + \"_\" + str(line[1]).zfill(4) + \".jpg\"),\n                    os.path.join(line[2], line[2] + \"_\" + str(line[3]).zfill(4) + \".jpg\"),\n                    0,\n                )\n                self.eval_list.append(eval_pair)\n            else:\n                pass\n\n        self.transform = transforms.Compose([\n            transforms.Resize(96),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5]),\n        ])\n\n    def __getitem__(self, index):\n        img_1_path, img_2_path, label = self.eval_list[index]\n\n        img_1 = Image.open(os.path.join(self.img_folder, img_1_path))\n        img_2 = Image.open(os.path.join(self.img_folder, img_2_path))\n        img_1 = self.transform(img_1)\n        img_2 = self.transform(img_2)\n\n        return img_1, img_2, label\n\n    def __len__(self):\n        return len(self.eval_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:36.139283Z","iopub.execute_input":"2024-03-16T02:03:36.139615Z","iopub.status.idle":"2024-03-16T02:03:36.154549Z","shell.execute_reply.started":"2024-03-16T02:03:36.139592Z","shell.execute_reply":"2024-03-16T02:03:36.153572Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# main.py","metadata":{}},{"cell_type":"code","source":"# TESTING AND RESULTS\n\ndef eval(data_loader: DataLoader, model: SphereCNN, device: torch.device, threshold: float = 0.5):\n    model.eval()\n    model.feature = True\n    sim_func = nn.CosineSimilarity()\n\n    cnt = 0.\n    total = 0.\n\n    t1 = time.time()\n    with torch.no_grad():\n        for img_1, img_2, label in data_loader:\n            img_1 = img_1.to(device)\n            img_2 = img_2.to(device)\n            label = label.to(device)\n\n            feat_1 = model(img_1, None)\n            feat_2 = model(img_2, None)\n            sim = sim_func(feat_1, feat_2)\n\n            sim[sim > threshold] = 1\n            sim[sim <= threshold] = 0\n\n            total += sim.size(0)\n            for i in range(sim.size(0)):\n                if sim[i] == label[i]:\n                    cnt += 1\n\n    print(\"Acc.: %.4f; Time: %.3f\" % (cnt / total, time.time() - t1))\n    return","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:03:36.157621Z","iopub.execute_input":"2024-03-16T02:03:36.157934Z","iopub.status.idle":"2024-03-16T02:03:36.169405Z","shell.execute_reply.started":"2024-03-16T02:03:36.157911Z","shell.execute_reply":"2024-03-16T02:03:36.168415Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"args = parse_args()\n\nset_seed(args.seed)\ndevice = torch.device(args.device)\n\n# DATA LOADING\n# size determination of train/validation sets\ntrain_set = LFW4Training(args.train_file, args.img_folder)\neval_set = LFW4Eval(args.eval_file, args.img_folder)\n\nvalidation_split = 0.2 # 20% of training data \n\neval_size = len(eval_set)\nval_size = int(validation_split * eval_size)\ntest_size = eval_size - val_size\n\n# creating datasets\ntest_set, val_set = torch.utils.data.random_split(eval_set, [test_size, val_size])\n\n# creating data loader\ntrain_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\neval_loader = DataLoader(test_set, batch_size=args.batch_size)\nval_loader = DataLoader(val_set, batch_size=args.batch_size)\n\n\n# INITIALIZE NEURAL NETWORK\nmodel = SphereCNN(class_num=train_set.n_label)\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\nloss_record = AverageMeter()\n\n# TRAINING DATA\nfor epoch in range(args.epoch):\n    t1 = time.time()\n    model.train()\n    model.feature = False\n    loss_record.reset()\n\n    for inputs, targets in train_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad()\n        _, loss = model(inputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        loss_record.update(loss)\n\n    print(\"Epoch: %s; Loss: %.3f; Time: %.3f\" % (str(epoch).zfill(2), loss_record.avg, time.time() - t1))\n\n    if (epoch + 1) % args.eval_interval == 0:\n        torch.save(model.state_dict(), \"/spherecnn.pth\")\n        eval(val_loader, model, device) #evaluate w validation set\n\n# TESTING\neval(eval_loader, model, device)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-16T02:05:16.155558Z","iopub.execute_input":"2024-03-16T02:05:16.156282Z","iopub.status.idle":"2024-03-16T02:29:39.263024Z","shell.execute_reply.started":"2024-03-16T02:05:16.156248Z","shell.execute_reply":"2024-03-16T02:29:39.261591Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch: 00; Loss: 7.628; Time: 27.243\nEpoch: 01; Loss: 7.352; Time: 11.793\nEpoch: 02; Loss: 7.076; Time: 12.298\nEpoch: 03; Loss: 6.918; Time: 12.197\nEpoch: 04; Loss: 6.833; Time: 11.618\nEpoch: 05; Loss: 6.781; Time: 12.123\nEpoch: 06; Loss: 6.745; Time: 11.809\nEpoch: 07; Loss: 6.715; Time: 12.110\nEpoch: 08; Loss: 6.686; Time: 12.747\nEpoch: 09; Loss: 6.655; Time: 11.967\nEpoch: 10; Loss: 6.623; Time: 12.193\nEpoch: 11; Loss: 6.589; Time: 11.560\nEpoch: 12; Loss: 6.553; Time: 11.893\nEpoch: 13; Loss: 6.518; Time: 12.206\nEpoch: 14; Loss: 6.482; Time: 11.649\nEpoch: 15; Loss: 6.443; Time: 12.604\nEpoch: 16; Loss: 6.406; Time: 11.764\nEpoch: 17; Loss: 6.367; Time: 12.103\nEpoch: 18; Loss: 6.328; Time: 12.554\nEpoch: 19; Loss: 6.289; Time: 12.413\nAcc.: 0.5550; Time: 2.653\nEpoch: 20; Loss: 6.250; Time: 13.035\nEpoch: 21; Loss: 6.212; Time: 12.230\nEpoch: 22; Loss: 6.173; Time: 12.579\nEpoch: 23; Loss: 6.134; Time: 12.543\nEpoch: 24; Loss: 6.095; Time: 12.351\nEpoch: 25; Loss: 6.056; Time: 12.622\nEpoch: 26; Loss: 6.017; Time: 12.152\nEpoch: 27; Loss: 5.978; Time: 12.050\nEpoch: 28; Loss: 5.940; Time: 13.145\nEpoch: 29; Loss: 5.905; Time: 12.399\nEpoch: 30; Loss: 5.868; Time: 12.412\nEpoch: 31; Loss: 5.829; Time: 12.282\nEpoch: 32; Loss: 5.793; Time: 12.146\nEpoch: 33; Loss: 5.759; Time: 12.145\nEpoch: 34; Loss: 5.722; Time: 12.314\nEpoch: 35; Loss: 5.685; Time: 12.323\nEpoch: 36; Loss: 5.652; Time: 12.240\nEpoch: 37; Loss: 5.612; Time: 12.203\nEpoch: 38; Loss: 5.573; Time: 12.036\nEpoch: 39; Loss: 5.538; Time: 12.152\nAcc.: 0.6000; Time: 1.116\nEpoch: 40; Loss: 5.503; Time: 11.844\nEpoch: 41; Loss: 5.468; Time: 14.424\nEpoch: 42; Loss: 5.439; Time: 12.097\nEpoch: 43; Loss: 5.402; Time: 12.250\nEpoch: 44; Loss: 5.366; Time: 12.098\nEpoch: 45; Loss: 5.335; Time: 11.866\nEpoch: 46; Loss: 5.298; Time: 12.101\nEpoch: 47; Loss: 5.260; Time: 11.883\nEpoch: 48; Loss: 5.221; Time: 12.598\nEpoch: 49; Loss: 5.184; Time: 19.580\nEpoch: 50; Loss: 5.148; Time: 19.868\nEpoch: 51; Loss: 5.112; Time: 22.589\nEpoch: 52; Loss: 5.079; Time: 23.605\nEpoch: 53; Loss: 5.050; Time: 20.001\nEpoch: 54; Loss: 5.021; Time: 20.058\nEpoch: 55; Loss: 4.984; Time: 22.680\nEpoch: 56; Loss: 4.959; Time: 23.774\nEpoch: 57; Loss: 4.936; Time: 20.272\nEpoch: 58; Loss: 4.898; Time: 20.067\nEpoch: 59; Loss: 4.854; Time: 22.753\nAcc.: 0.6300; Time: 4.539\nEpoch: 60; Loss: 4.822; Time: 23.916\nEpoch: 61; Loss: 4.782; Time: 20.699\nEpoch: 62; Loss: 4.739; Time: 21.290\nEpoch: 63; Loss: 4.709; Time: 22.867\nEpoch: 64; Loss: 4.683; Time: 22.071\nEpoch: 65; Loss: 4.654; Time: 16.996\nEpoch: 66; Loss: 4.615; Time: 16.881\nEpoch: 67; Loss: 4.586; Time: 20.584\nEpoch: 68; Loss: 4.547; Time: 21.919\nEpoch: 69; Loss: 4.507; Time: 19.949\nEpoch: 70; Loss: 4.467; Time: 22.488\nEpoch: 71; Loss: 4.432; Time: 28.861\nEpoch: 72; Loss: 4.407; Time: 20.675\nEpoch: 73; Loss: 4.379; Time: 20.492\nEpoch: 74; Loss: 4.344; Time: 24.759\nEpoch: 75; Loss: 4.308; Time: 20.252\nEpoch: 76; Loss: 4.282; Time: 18.316\nEpoch: 77; Loss: 4.251; Time: 23.848\nEpoch: 78; Loss: 4.219; Time: 23.222\nEpoch: 79; Loss: 4.184; Time: 20.259\nAcc.: 0.6500; Time: 1.543\nEpoch: 80; Loss: 4.147; Time: 25.192\nEpoch: 81; Loss: 4.120; Time: 21.817\nEpoch: 82; Loss: 4.095; Time: 23.359\nEpoch: 83; Loss: 4.052; Time: 23.614\nEpoch: 84; Loss: 4.011; Time: 24.292\nEpoch: 85; Loss: 3.979; Time: 21.851\nEpoch: 86; Loss: 3.955; Time: 20.342\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mfeature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     39\u001b[0m loss_record\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     42\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mLFW4Training.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     33\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_list[index]\n\u001b[0;32m---> 35\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m     38\u001b[0m     name \u001b[38;5;241m=\u001b[39m img_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}